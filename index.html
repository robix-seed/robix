      
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Robix | ByteDance Seed</title>
  <link rel="icon" type="image/x-icon" href="static/images/bytedance.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Robix: A Unified Model for <br> Robot Interaction, Reasoning and Planning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Huang Fang<sup>*</sup>,</span>
                <span class="author-block">
                  Mengxi Zhang<sup>*</sup>,</span>
                  <span class="author-block">
                    Heng Dong<sup>*</sup>,</span>
                    <span class="author-block">
                      Wei Li<sup>*&dagger;</sup>,</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- Paper authors -->
                    <span class="author-block">
                      Zixuan Wang,</span>
                      <span class="author-block">
                       Qifeng Zhang,</span>
                        <span class="author-block">
                          Xueyun Tian,</span>
                          <span class="author-block">
                            Yucheng Hu,</span>
                            <span class="author-block">
                              Hang Li<sup>&dagger;</sup>,</span>
                        </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">ByteDance Seed<br></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup> Equal Contribution  <sup>&dagger;</sup> Project Lead</small></span>
                  <span class="eql-cntrb"><small><br><strong>Correspondence:</strong> liwei.85@bytedance.com, lihang.lh@bytedance.com</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Demos Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-laptop"></i> <!-- 电脑图标 -->
                  </span>
                  <span>Demo</span>
                </a>
              </span>

            <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i> <!-- 电脑图标 -->
                  </span>
                  <span>Arxiv</span>
                </a>
              </span>

                          
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
<!--       <iframe width="960" height="540" 
        src="https://www.youtube.com/embed/vEDEuhKaIfU?loop=1&playlist=vEDEuhKaIfU&modestbranding=1&showinfo=0&rel=0&controls=1&start=20" 
        frameborder="0" 
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
        allowfullscreen>
      </iframe> -->
      <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/vEDEuhKaIfU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
       </div>
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop" style="margin: 0 auto;">
    <div class="columns is-centered has-text-centered">
      <div class="column" style="width: 100%;">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce <strong>Robix</strong>, a unified model that integrates robot reasoning, task planning, and natural language interaction within a single vision-language architecture. Acting as the high-level cognitive layer in a hierarchical robot system, Robix dynamically generates atomic commands for the low-level controller and verbal responses for human interaction, enabling robots to follow complex instructions, plan long-horizon tasks, and interact naturally with human within an end-to-end framework. Robix further introduces novel capabilities such as proactive dialogue, real-time interruption handling, and context-aware commonsense reasoning during task execution. At its core, Robix leverages chain-of-thought reasoning and adopts a three-stage training strategy: (1) continued pretraining to enhance foundational embodied reasoning abilities including 3D spatial understanding, visual grounding, and task-centric reasoning; (2) supervised finetuning to model human-robot interaction and task planning as a unified reasoning-action sequence; and (3) reinforcement learning to improve reasoning-action consistency and long-horizon task coherence. Extensive experiments demonstrate that Robix outperforms both open-source and commercial baselines (<em>e.g</em>, GPT-4o and Gemini 2.5 Pro) in interactive task execution, demonstrating strong generalization across diverse instruction types (<em>e.g</em>., open-ended, multi-stage, constrained, invalid, and interrupted) and various user-involved tasks such as table bussing, grocery shopping, and dietary filtering.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<hr style="width: 52%;         /* 控制长度，50% 宽度 */
           height: 3px;       /* 控制线宽 */
           background-color: #ccc; /* 灰色 */
           border: none;       /* 去掉默认边框 */
           margin: 20px auto;">  <!-- 上下间距 + 居中 -->

<section class="hero is-small">
  <div class="hero-body">
  <div class="container">
    <div class="container has-text-centered">
      <h2 class="title is-3">Overview of Robix</h2>

      <div style="background-color: #f0f0f0; 
            padding: 12px; 
            border-radius: 6px; 
            max-width: 900px; /* 关键：必须有固定宽度/最大宽度，margin:0 auto才生效 */
            margin: 0 auto; /* 核心居中属性：上下边距0，左右自动（实现水平居中） */
            box-sizing: border-box;
            text-align: left;"> <!-- 可选：让padding不撑大容器宽度，避免超出max-width -->
          <h2 class="title is-5" style="margin-bottom: 8px;">
              <strong>The main features of Robix are summarized as follows:</strong>
          </h2>
          <ul style="list-style: none; padding-left: 0; margin: 0;">
            <li style="margin: 2px; padding: 0; line-height: 1.5;">🌟&nbsp;<strong>Unified model</strong>. Robix is a single vision-language model that unifies robot reasoning, task planning, and human-robot interaction, enabling robots to follow complex instructions, plan long-horizon tasks, and interact naturally in an end-to-end manner.</li>
            <li style="margin: 2px; padding: 0; line-height: 1.5;">🌟&nbsp;<strong>Flexible interaction</strong>. Within this unified framework, Robix supports proactive dialogue to clarify ambiguity and infer user intent, real-time interruption handling that seamlessly incorporates feedback, and context-aware commonsense reasoning for complex, open-ended tasks.</li>
            <li style="margin: 2px; padding: 0; line-height: 1.5;">🌟&nbsp;<strong>Robust Performance</strong>. We assess Robix in two setups: (i) on a curated interactive-task benchmark covering both in- and out-of-distribution scenarios with diverse instruction types, and (ii) across five real-world scenarios in a hierarchical robot system with both human teleoperation and an automatic VLA model as the low-level controller. These evaluations demonstrate that Robix consistently delivers strong performance across all settings.</li>
          </ul>
      </div>

      <br>
      
      <img src="static/images/online_images/demo-cases.jpg" alt="MY ALT TEXT" style="max-width: 900px; width: 100%; height: auto; margin: 0 auto;"/>
      <h2 class="subtitle is-size-6" style="max-width: 900px; margin: 0 auto; text-align: left;">
        A demo of Robix, showcasing (1) complex instruction understanding with commonsense reasoning; (2) real-time interruption handling; (3) task-status monitoring and dynamic replanning; and (4) proactive dialogue to clarify ambiguous instructions or infer user intent.
      </h2>
      </div>
    </div>
  </div>
</section>

<hr style="width: 52%;         /* 控制长度，50% 宽度 */
           height: 3px;       /* 控制线宽 */
           background-color: #ccc; /* 灰色 */
           border: none;       /* 去掉默认边框 */
           margin: 20px auto;">  <!-- 上下间距 + 居中 -->


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="container has-text-centered">
      <h2 class="title is-3">Architecture of Robix</h2>
      <div style="text-align: left;    /* 左对齐 */
          max-width: 900px;     /* 最大宽度 */
          margin: 0 auto;       /* 居中整个块，可选 */
          line-height: 1.6;">   
         The figure below illustrates the hierarchical robot system, where Robix serves as the high-level cognitive layer, interpreting tasks and reasoning over multimodal inputs to generate language responses and action plans, while the low-level controller—typically a vision–language–action (VLA) model—executes the atomic commands produced by Robix. This hierarchical design enables the robot to interact seamlessly with both humans and the physical environment.
      </div>
      <br>
      <img src="static/images/online_images/model-architecture.png" alt="MY ALT TEXT" style="max-width: 900px; width: 100%; height: auto; margin: 0 auto;"/>
      <h2 class="subtitle has-text-centered is-size-6" style="max-width: 900px; margin: 0 auto;">
        Illustration of the hierarchical robot system.
      </h2>
      <br>
      <div style="text-align: left;    /* 左对齐 */
          max-width: 900px;     /* 最大宽度 */
          margin: 0 auto;       /* 居中整个块，可选 */
          line-height: 1.6;">   
         At each iteration, Robix directly processes visual observations from robot-mounted cameras and user utterances, selectively producing atomic action commands for the low-level controller and appropriate verbal responses. This iterative reasoning-action loop allows Robix to perform deliberate reasoning and generate contextually grounded behaviors.
      </div>
    </div>
  </div>
  </div>
</section>

<hr style="width: 52%;         /* 控制长度，50% 宽度 */
           height: 3px;       /* 控制线宽 */
           background-color: #ccc; /* 灰色 */
           border: none;       /* 去掉默认边框 */
           margin: 20px auto;">  <!-- 上下间距 + 居中 -->


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="container has-text-centered">
      <h2 class="title is-3"> Fundamental Perception &amp; Reasoning Evaluation</h2>
      <div style="text-align: left;    /* 左对齐 */
            max-width: 900px;     /* 最大宽度 */
            margin: 0 auto;       /* 居中整个块，可选 */
            line-height: 1.6;">   
          We first evaluate the fundamental perception and reasoning capabilities of Robix on a comprehensive suite of public benchmarks, comparing it against state-of-the-art multimodal models including Qwen-2.5-VL-7B & 32B, RoboBrain-2.0-32B, Cosmos-Reason1-7B, Gemini-2.5-Pro, GPT-4o, Seed-1.5-VL, and Seed-1.5-VL-Think. The evaluation covers (1) robotics-relevant embodied reasoning—including 3D spatial understanding, visual grounding, and task-centric reasoning—and (2) general multimodal understanding and reasoning.
        </div>
        <br>
        <img src="static/images/online_images/tables.png" alt="MY ALT TEXT" style="max-width: 900px; width: 100%; height: auto; margin: 0 auto;"/>
        <h2 class="subtitle is-size-6" style="max-width: 900px; margin: 0 auto; text-align: left;">
          Performance of Robix on public vision-language benchmarks compared to prior models. The left side shows Robix and state-of-the-art open-source baselines, while the right side presents closed-source large commercial models. The highest score in each benchmark is highlighted in bold within each group.
        </h2>
        
      </div>
    
  </div>
  </div>
</section>

<div style="background-color: #f0f0f0; 
            padding: 12px; 
            border-radius: 6px; 
            max-width: 900px;
            margin: 0 auto;">   <!-- 关键在这里 -->
  <h2 class="title is-5" style="margin-bottom: 8px;">
      <strong>Results</strong>
  </h2>
  <ul style="list-style: none; padding-left: 0; margin: 0;">
    <li style="margin: 2px; padding: 0; line-height: 1.5;">🌟&nbsp;<strong>3D Spatial Understanding</strong>. Robix-7B and Robix-32B outperform Qwen2.5-VL-7B/32B on 7 of 8 spatial reasoning tasks, with average accuracies of 73.4 and 75.8 compared to 66.9 and 70.7, respectively. They also surpass embodied models Cosmos-Reason1-7B (64.0) and RoboBrain-32B (72.2), and exceed the best commercial baseline, Gemini-2.5-Pro, on 5 of 8 tasks.</li>
    <li style="margin: 2px; padding: 0; line-height: 1.5;">🌟&nbsp;<strong>Visual Grounding</strong>. Across 8 visual grounding benchmarks, Robix consistently outperforms Qwen2.5-VL across all benchmarks and surpasses state-of-the-art commercial models on most tasks. Notably, Robix-7B and Robix-32B improve the absolute F1 score on LVIS-MG by 39.6 and 25.0 points over Qwen2.5-VL-7B and 32B, respectively. These results highlight Robix’s strong performance in object localization, pointing and fine-grained visual understanding.</li>
    <li style="margin: 2px; padding: 0; line-height: 1.5;">🌟&nbsp;<strong>Embodied Task-centric Reasoning</strong>. Robix consistently outperforms its backbone models as well as Cosmos-Reason1-7B and RoboBrain-2.0-32B across most benchmarks. On Agibot-ER, Robix delivers substantial gains over its backbones, improving absolute accuracy by 12.8 and 7.2 points for the 7B and 32B versions, respectively. It further surpasses Cosmos-Reason1-7B and RoboBrain-2.0-32B by 23 and 8.3 points, demonstrating superior performance in embodied, task-centric reasoning.</li>
  </ul>
</div>


<hr style="width: 52%;         /* 控制长度，50% 宽度 */
           height: 3px;       /* 控制线宽 */
           background-color: #ccc; /* 灰色 */
           border: none;       /* 去掉默认边框 */
           margin: 20px auto;">  <!-- 上下间距 + 居中 -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="container has-text-centered">
      <h2 class="title is-3"> Offline Evaluation of Robix</h2>
      <div style="text-align: left;    /* 左对齐 */
          max-width: 900px;     /* 最大宽度 */
          margin: 0 auto;       /* 居中整个块，可选 */
          line-height: 1.6;">   
       The offline evaluation enables fully automated assessment of planning and interaction capabilities using predefined evaluation sets. To thoroughly evaluate both interactive long-horizon planning and out-of-distribution (OOD) generalization, we design three dedicated evaluation sets:
       <ul style="list-style-type: disc; margin-left: 20px;">
        <li><strong>AGIBot Evaluation Set:</strong> We manually select 16 high-frequency daily tasks from the AGIBot dataset and ensure none appear in the training data. This set primarily evaluates the model’s long-horizon task planning capability on OOD tasks.</li>
        <li><strong>Internal OOD Benchmark:</strong> We manually design 16 scripts covering task planning and diverse human–robot interaction scenarios, including table organization, dietary filtering, checkout packing, grocery shopping, and shoe cabinet organization. The benchmark includes tasks and items absent from the training data and is intended to evaluate interactive task execution in unseen scenarios.</li>
        <li><strong>Internal ID Benchmark:</strong> This evaluation set is randomly sampled from our synthesized data and categorized by task type and user instruction into six groups. Each category targets evaluation of the model’s corresponding instruction following and task planning capabilities.</li>
      </ul>

      </div>
      <br>
      <img src="static/images/online_images/tables_offline.png" alt="MY ALT TEXT" style="max-width: 900px; width: 100%; height: auto; margin: 0 auto;"/>
      <h2 class="subtitle is-size-6" style="max-width: 900px; margin: 0 auto; text-align: left;">
        Offline evaluation results. Robix-7B-SFT-wo-R refers to our SFT model without chain-of-thought reasoning, while Robix-7B-RL denotes the full trained policy obtained by applying RL after SFT. For AGIBot, Internal OOD, and Internal ID–MultiStage/Constrained/Interrupt/OpenEnded, we report plan accuracy; for Internal ID–Invalid/Replan, we report F1 score. The best result for each evaluation set is shown in bold, and the best among baselines is underlined.
      </h2>
    </div>
  </div>
  </div>
</section>


<div style="background-color: #f0f0f0; 
            padding: 12px; 
            border-radius: 6px; 
            max-width: 900px;
            margin: 0 auto;">   <!-- 关键在这里 -->
  <h2 class="title is-5" style="margin-bottom: 8px;">
      <strong>Results</strong>
  </h2>
  <ul style="list-style: none; padding-left: 0; margin: 0;">
    <li style="margin: 2px; padding: 0; line-height: 1.5;">🌟&nbsp;<strong>Overall Performance</strong>. Robix-32B-RL ranks first on all evaluation sets, demonstrating strong task planning and human–robot interaction capabilities, and substantially outperforming all open-source and commercial VLMs on both ID and OOD benchmarks.</li>
    <li style="margin: 2px; padding: 0; line-height: 1.5;">🌟&nbsp;<strong>OOD Generalization</strong>. On the two OOD evaluation benchmarks (AgiBot and Internal-OOD), Robix-32B achieves accuracy improvements of 11.8 and 3.0 over Gemini-2.5-Pro, highlighting its superior generalization ability.</li>
    <li style="margin: 2px; padding: 0; line-height: 1.5;">🌟&nbsp;<strong>Chain-of-Thought Reasoning</strong>. Chain-of-Thought reasoning substantially enhances model performance on OOD generalization and complex instruction understanding.</li>
    <li style="margin: 2px; padding: 0; line-height: 1.5;">🌟&nbsp;<strong>Reinforcement Learning</strong>. Compared with the SFT-only counterpart, Robix-7B-RL and Robix-32B-RL achieve accuracy gains of 8.3 and 3.3 improvements on the challenging Internal OOD benchmark, respectively.</li>
  </ul>
</div>

<hr style="width: 52%;         /* 控制长度，50% 宽度 */
           height: 3px;       /* 控制线宽 */
           background-color: #ccc; /* 灰色 */
           border: none;       /* 去掉默认边框 */
           margin: 20px auto;">  <!-- 上下间距 + 居中 -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="container has-text-centered">
      <h2 class="title is-3">Online Evaluation of Robix</h2>
      <div style="text-align: left;    /* 左对齐 */
          max-width: 900px;     /* 最大宽度 */
          margin: 0 auto;       /* 居中整个块，可选 */
          line-height: 1.6;">   
       We deploy Robix within a hierarchical robot system across diverse real-world tasks, including: <br>

       <ul style="list-style-type: disc; margin-left: 20px;">
        <li><strong>Table Bussing:</strong> Clearing used tableware, utensils, and food items.</li>
        <li><strong>Checkout Packing:</strong> Organizing purchased goods during checkout and placing them into bags or boxes.</li>
        <li><strong>Dietary Filtering:</strong> Selecting or excluding food and beverages according to dietary restrictions (e.g., caffeine-free).</li>
        <li><strong>Grocery Shopping:</strong> Recommending and purchasing grocery items based on user instructions.</li>
        <li><strong>Tableware Organization & Shipment:</strong> Categorizing, packing tableware and transporting it to designated locations.</li>
      </ul>
      <br>
      We design two sets of experiments:
      <br>
      (1) In the first set of experiments, VLMs serve as the high-level planning and interaction module, while human labelers equipped with a UMI device act as the low-level controller, enabling evaluation under a fully reliable control setting. 
      </div>
      <br>
      <img src="static/images/online_images/online_exp_2row_blue.png" alt="MY ALT TEXT" style="max-width: 900px; width: 100%; height: auto; margin: 0 auto;"/>
      <h2 class="subtitle has-text-centered is-size-6" style="max-width: 900px; margin: 0 auto;">
        Online evaluation results with a human labeler operating a UMI device as the low-level controller.
      </h2>
    </div>
  </div>
  </div>
    <div class="container" style="text-align: left;    /* 左对齐 */
          max-width: 900px;     /* 最大宽度 */
          margin: 0 auto;       /* 居中整个块，可选 */
          line-height: 1.6;">
       (2) In the second set, we use our in-house VLA model GR-3, as the low-level controller and deploy the integrated VLM–VLA system on the ByteMini robot.

      <div class="container has-text-centered">
      <img src="static/images/online_images/bar_chart_vla_blue.png" alt="MY ALT TEXT" style="max-width: 900px; width: 100%; height: auto; margin: 0 auto;"/>
      <h2 class="subtitle has-text-centered is-size-6" style="max-width: 900px; margin: 0 auto;">
        Online evaluation on the ByteMini robot with GR-3 model as the low-level controller.
      </h2>
      </div>
  </div>
  </div>
</section>

<br>

<div style="background-color: #f0f0f0; 
            padding: 12px; 
            border-radius: 6px; 
            max-width: 900px; 
            margin: 0 auto;">   <!-- 关键在这里 -->
  <h2 class="title is-5" style="margin-bottom: 8px;">
      <strong>Results</strong>
  </h2>
  <ul style="list-style: none; padding-left: 0; margin: 0;">
    <li style="margin: 2px; padding: 0; line-height: 1.5;">🌟&nbsp;<strong>Online evaluation with UMI</strong>. Under the UMI setting, both Robix-32B and Gemini-2.5-Pro rank first in 3 of 5 tasks, with Robix-32B achieving a slightly higher average task progress (92.6% vs. 91%), highlighting its superior performance in dynamic real-world environments. Moreover, Robix-32B surpasses Qwen2.5-VL-32B by a substantial margin (92.6% vs. 28%), underscoring the effectiveness of our training pipeline.</li>
    <li style="margin: 2px; padding: 0; line-height: 1.5;">🌟&nbsp;<strong>Online evaluation with GR-3</strong>. In automated real-robot evaluation, Robix-32B achieves an average task progress of 92.5%, surpassing Gemini-2.5-Pro and GPT-4o by 4.3 and 28.1 percentage points, respectively.</li>
  </ul>
</div>



<hr style="width: 52%;         /* 控制长度，50% 宽度 */
           height: 3px;       /* 控制线宽 */
           background-color: #ccc; /* 灰色 */
           border: none;       /* 去掉默认边框 */
           margin: 20px auto;">  <!-- 上下间距 + 居中 -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{fang2025robix,
    title = {Robix: A Unified Model for Robot Interaction, Reasoning and Planning},
    author = {Huang Fang, Mengxi Zhang, Heng Dong, Wei Li, Zixuan Wang, Qifeng Zhang, Xueyun Tian, Yucheng Hu, Hang Li},
    journal = {},
    year = {2025}
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <!-- <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer> -->

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>

    
